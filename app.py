from flask import Flask, request, jsonify, render_template, send_from_directory
from flask_cors import CORS
import PyPDF2
import pdfplumber
import re
import os
from datetime import datetime
import hashlib
import traceback

app = Flask(__name__, static_folder="static", template_folder="templates")
CORS(app)

# Ù…Ø¬Ù„Ø¯ Ø§Ù„ØªØ­Ù…ÙŠÙ„
UPLOAD_FOLDER = 'uploads'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB Ø­Ø¯ Ø£Ù‚ØµÙ‰

class PDFProcessor:
    def __init__(self, file_path):
        self.file_path = file_path
        self.text_content = ""
        self.metadata = {}
        self.tables = []
        self.entities = []
        
    def extract_basic_info(self):
        try:
            with open(self.file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                self.metadata = {
                    'pages': len(pdf_reader.pages),
                    'title': pdf_reader.metadata.get('/Title', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
                    'author': pdf_reader.metadata.get('/Author', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
                    'creation_date': pdf_reader.metadata.get('/CreationDate', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
                    'file_size': f"{os.path.getsize(self.file_path) / 1024:.2f} KB"
                }
                print(f"ğŸ“Š Basic info: {self.metadata['pages']} pages")
        except Exception as e:
            print(f"âŒ Error in basic info: {e}")
            self.metadata = {'pages': 0, 'title': 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯', 'author': 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯', 
                           'creation_date': 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯', 'file_size': 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ'}
    
    def extract_text_and_tables(self):
        try:
            with pdfplumber.open(self.file_path) as pdf:
                self.text_content = ""
                self.tables = []
                
                for page_num, page in enumerate(pdf.pages):
                    text = page.extract_text() or ""
                    self.text_content += f"\n--- Ø§Ù„ØµÙØ­Ø© {page_num + 1} ---\n{text}\n"
                    
                    page_tables = page.extract_tables() or []
                    for table_num, table in enumerate(page_tables):
                        if table:
                            table_data = {
                                'page': page_num + 1,
                                'table_number': table_num + 1,
                                'headers': [str(cell) if cell is not None else '' for cell in table[0]] if table else [],
                                'rows': [[str(cell) if cell is not None else '' for cell in row] for row in table[1:]] if len(table) > 1 else []
                            }
                            self.tables.append(table_data)
        except Exception as e:
            print(f"âŒ Error extracting text: {e}")
            self.text_content = "ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ù…Ù„Ù"
    
    def extract_entities(self):
        if not self.text_content:
            return
            
        patterns = {
            'Ø£Ø³Ù…Ø§Ø¡': r'\b[Ø£-ÙŠ]{3,}\s[Ø£-ÙŠ]{3,}\b',
            'ØªÙˆØ§Ø±ÙŠØ®': r'\b\d{1,2}/\d{1,2}/\d{2,4}\b|\b\d{4}-\d{2}-\d{2}\b',
            'Ø£Ø±Ù‚Ø§Ù… Ù‡ÙˆØ§ØªÙ': r'\b\d{10,15}\b',
            'Ø¨Ø±ÙŠØ¯ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'Ø¹Ù†ÙˆØ§Ù† ÙˆÙŠØ¨': r'https?://[^\s]+',
            'Ø£Ø±Ù‚Ø§Ù…': r'\b\d+\b'
        }
        
        for entity_type, pattern in patterns.items():
            try:
                matches = re.finditer(pattern, self.text_content)
                for match in matches:
                    self.entities.append({
                        'type': entity_type,
                        'text': match.group(),
                        'confidence': min(95, 70 + len(match.group()) * 2)
                    })
            except:
                continue
    
    def analyze_semantic_content(self):
        if not self.text_content:
            return {
                'documentType': 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ',
                'topics': [],
                'language': 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ',
                'summary': 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø­ØªÙˆÙ‰ Ù„ØªØ­Ù„ÙŠÙ„Ù‡'
            }
        
        try:
            document_type = "Ø¹Ø§Ù…"
            text_lower = self.text_content.lower()
            
            if any(word in text_lower for word in ['Ø¹Ù‚Ø¯', 'Ø§ØªÙØ§Ù‚ÙŠØ©', 'Ù…Ø§Ø¯Ø©', 'Ø¨Ù†Ø¯']):
                document_type = "Ø¹Ù‚Ø¯"
            elif any(word in text_lower for word in ['ÙØ§ØªÙˆØ±Ø©', 'Ø³Ø¹Ø±', 'ÙƒÙ…ÙŠØ©', 'Ù…Ø¬Ù…ÙˆØ¹']):
                document_type = "ÙØ§ØªÙˆØ±Ø©"
            elif any(word in text_lower for word in ['ØªÙ‚Ø±ÙŠØ±', 'ØªØ­Ù„ÙŠÙ„', 'Ù†ØªÙŠØ¬Ø©']):
                document_type = "ØªÙ‚Ø±ÙŠØ±"
            
            topics = self.extract_topics()
            language = "Ø¹Ø±Ø¨ÙŠØ©" if re.search(r'[Ø£-ÙŠ]', self.text_content) else "Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©"
            sentences = [s.strip() for s in self.text_content.split('.') if s.strip()]
            summary = '. '.join(sentences[:3]) + '.' if len(sentences) > 3 else self.text_content[:200] + '...'
            
            return {
                'documentType': document_type,
                'topics': topics[:5],
                'language': language,
                'summary': summary
            }
        except:
            return {
                'documentType': 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ',
                'topics': [],
                'language': 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ',
                'summary': 'Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„'
            }
    
    def extract_topics(self):
        try:
            common_arabic_words = {
                'Ø¹Ù…Ù„', 'Ø´Ø±ÙƒØ©', 'Ù…Ø´Ø±ÙˆØ¹', 'Ø¯Ø±Ø§Ø³Ø©', 'ØªØ­Ù„ÙŠÙ„', 'ØªÙ‚Ø±ÙŠØ±', 'Ø¹Ù‚Ø¯', 'Ø§ØªÙØ§Ù‚',
                'Ø¯ÙØ¹', 'Ø³Ø¹Ø±', 'ØªÙƒÙ„ÙØ©', 'Ù…ÙˆØ¹Ø¯', 'ØªØ§Ø±ÙŠØ®', 'Ù…Ø¨Ù„Øº', 'Ø¹Ù…Ù„ÙŠØ©', 'Ù†Ø¸Ø§Ù…'
            }
            
            words = re.findall(r'\b[Ø£-ÙŠ]{3,}\b', self.text_content)
            word_freq = {}
            
            for word in words:
                if word in common_arabic_words:
                    word_freq[word] = word_freq.get(word, 0) + 1
            
            sorted_topics = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
            return [topic[0] for topic in sorted_topics[:10]]
        except:
            return []
    
    def process(self):
        try:
            print("ğŸ”„ Starting PDF processing...")
            
            self.extract_basic_info()
            self.extract_text_and_tables()
            self.extract_entities()
            semantic_analysis = self.analyze_semantic_content()
            
            result = {
                'success': True,
                'basicInfo': self.metadata,
                'rawText': self.text_content[:5000] if self.text_content else 'Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ù†Øµ',
                'tables': self.tables[:5],
                'entities': self.entities[:20],
                'semanticAnalysis': semantic_analysis,
                'processingTime': datetime.now().isoformat()
            }
            
            print("âœ… Processing completed successfully!")
            return result
            
        except Exception as e:
            print(f"âŒ Error in processing: {e}")
            return {
                'success': False,
                'error': f'ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {str(e)}'
            }

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/<path:filename>')
def serve_static(filename):
    return send_from_directory(app.static_folder, filename)

@app.route('/analyze', methods=['POST'])
def analyze_pdf():
    try:
        if 'pdf' not in request.files:
            return jsonify({'success': False, 'error': 'Ù„Ù… ÙŠØªÙ… ØªÙ‚Ø¯ÙŠÙ… Ù…Ù„Ù'}), 400
        
        file = request.files['pdf']
        if file.filename == '':
            return jsonify({'success': False, 'error': 'Ù„Ù… ÙŠØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ù…Ù„Ù'}), 400
        
        if not file.filename.lower().endswith('.pdf'):
            return jsonify({'success': False, 'error': 'Ø§Ù„Ù…Ù„Ù ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø¨ØµÙŠØºØ© PDF'}), 400
        
        file_data = file.read()
        if len(file_data) == 0:
            return jsonify({'success': False, 'error': 'Ø§Ù„Ù…Ù„Ù ÙØ§Ø±Øº'}), 400
            
        file_hash = hashlib.md5(file_data).hexdigest()
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{file_hash}.pdf")
        
        file.seek(0)
        file.save(file_path)
        
        processor = PDFProcessor(file_path)
        result = processor.process()
        
        try:
            os.remove(file_path)
        except:
            pass
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'success': False, 'error': f'ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {str(e)}'}), 500

@app.route('/health')
def health_check():
    return jsonify({'status': 'healthy', 'message': 'Ø®Ø§Ø¯Ù… PDF Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„'})

if __name__ == '__main__':
    print("ğŸš€ Starting PDF Analysis Server...")
    app.run(debug=True, host='0.0.0.0', port=5000)
